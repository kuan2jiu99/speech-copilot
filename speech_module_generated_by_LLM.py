def query_llm(question: str) -> str:
    # Objective: Answer a text question using a large language model (default: gpt-3.5-turbo).
    # Input: question (str)
    # Output: the answer to the question generated by large language model (str).
    # Please note that the response from LLM may be a descriptive sentence, so it is not appropriate to do directly exact match.
    # If you need to compare two responses, you have to query LLM twice and compare the two responses with suitable prompts.
    # Example usage 1: 
    # If you want to ask "What is the capital of France?", you can call this function as follows:
    # answer = query_llm("What is the capital of France?")
    # Example usage 2:
    # You may also combine this function with other functions or models for further applications. 
    # For instance, you can use this to extract the answer from the descriptive response of the other models like this:
    # def execute_command(response, options) -> str:
    #     # response is the output of the other model. options are the possible answers allowed. The following code will extract the answer from the response.
    #     query = "The response is f'{response}' and the options are f'{options}'. Please select the only one correct option. Do not explain."
    #     answer = query_llm(query)
    #     return answer
    # In practice, you will frequently use this function to extract or map the output of other function to a list of allowed options. Make sure you understand how to do this.

def speech_recognition(audio_path: str) -> str:
    # Objective: Transcribe the spoken words in an audio file to text.
    # Input: audio_path (str) - the path to the audio file.
    # Output: transcription (str) - the transcribed text from the audio.
    # Example usage1:
    # If you want to transcribe an audio file, you can call this function as follows:
    # transcription = speech_recognition("path/to/audio/file.wav")
    # Example usage2:
    # If you want to identify the location where the speech is happening, you may extract the spoken information from the transcription and query LLM to map the location to the possible answers.
    # For example, you can use the following code to achieve this:
    # def execute_command(audio) -> str:
    #     # audio is the input audio files.
    #     transcription = speech_recognition(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the transcription: f'{{transcription}}' and the options are f'{{options}}', what is the location behind the speech?"
    #     # use query_llm to extract the location from the transcription.
    #     answer = query_llm(query)
    #     return answer
    # Example usage3:
    # If you want to identify the object related to the spoken message, you may extract the spoken information from the transcription and query LLM to map the object to the possible answers.
    # For example, you can use the following code to achieve this:
    # def execute_command(audio) -> str:
    #     # audio is the input audio files.
    #     transcription = speech_recognition(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the transcription: f'{{transcription}}' and the options are f'{{options}}', what is the object related to the speech?"
    #     # use query_llm to extract the location from the transcription.
    #     answer = query_llm(query)
    #     return answer

def sound_classification(audio_path: str, instruction: str) -> bool:
    # Objective: Classify the sound in the audio (not including the attributes in the speech) and output descriptive sentences.
    # Input: audio_path: an audio, instruction: the task instruction.
    # Output: the descriptive label (str).
    # Please note that the output of this function is a descriptive label. You need to query LLM (via query_llm function) to extract the specific information from the caption.
    # You should also notice that as this module will conduct the classification for several kinds of sounds, you need to provide the instruction to specify the task.
    # What this module can do:
    # - Classify different kinds of audio sounds (e.g., dog barking, rain, car horn).
    # - Distinguish objects that produce sounds (e.g., animals, natural sounds, human sounds, environmental sounds).
    # What this module cannot do:
    # - Identify higher-level concepts like intent or specific details about speech.
    # - For example, you can not classify the location behind the speech using this module. When processing the speech, you have to seek speech-related modules.
    # - Answer general questions beyond its scope of supported sounds.
    # - Identify the object or location in the spoken message. You should consider other speech-related modules.
    # The instruction is exactly the task instruction you faced, and if you receive possible options from the user, you should include the option list in the instruction, and don't make any modifications.
    # Example usage: 
    # For example, if you receive the user instruction: "Identify whether the sound is a bird or not. The possible choices are True or False.", and you think you should use this module.
    # You can use the following code to achieve this:
    # # the instruction for the second argument is what you recieved from the user. You should modify it based on the user's instruction. Here's only an example.
    # audio_caption = sound_classification(audio, "Identify whether the sound is a bird or not. The possible choices are True or False.")
    # # query based on the example question. You should modify this based on your specific task.
    # options = """True, False"""
    # query = f"Based on the information: f'{{audio_caption}}' and the options are f'{{options}}', what is the sound in the audio? Please select the only one correct option."
    # # use query_llm to extract the sound from the descriptive caption.
    # answer = query_llm(query)
    # return answer

def speaker_diarization(audio_path: str) -> Dict[str, Dict[str, Union[float, str]]]:
    # Objective: Identify the speakers in the audio.
    # Input: an audio, 
    # Output: dictionary (Dict[str, Dict[str, Union[float, str]]]).
    # The output dictionary will have the following format:
    # {
    #     "0": {
    #         "start": 0.0,
    #         "end": 2.5,
    #         "speaker": "speaker_0"
    #     },
    #     "1": {
    #         "start": 3.5,
    #         "end": 5.0,
    #         "speaker": "speaker_1"
    #     },
    #     "2": {
    #         "start": 5.5,
    #         "end": 7.0,
    #         "speaker": "speaker_0"
    #     },
    #     ...
    # }
    # Where the "start" and "end" is the starting and ending time of the speaker's speech, and "speaker" is the speaker's id.
    # You can use this information to identify the number of speakers in the audio and their respective speech segments.
    # Example usage 1:
    # If you want to identify the speaker of the first speech utterance, you can use the function as follows:
    # def execute_command(audio) -> int:
    #     # audio is the input audio files.
    #     diarization_result = speaker_diarization(audio)
    #     speaker_id = diarization_result["0"]["speaker"]
    #     return speaker_id
    # Example usage 2:
    # If you want to identify the number of unique speakers in the audio, you can use the function as follows:
    # def execute_command(audio) -> int:
    #     # audio is the input audio files.
    #     diarization_result = speaker_diarization(audio)
    #     speakers = []
    #     for key in diarization_result.keys():
    #       speaker = diarization_result[key]["speaker"]
    #       if speaker not in speakers:
    #         speakers.append(speaker)
    #     return len(speakers)
    # Example usage 3:
    # If you want to check whether there are more than one speaker in the audio, you can use the function as follows:
    # def execute_command(audio) -> bool:
    #     # audio is the input audio files. The output is True if there are more than one speaker in the audio, False otherwise.
    #     diarization_result = speaker_diarization(audio)
    #     speakers = []
    #     for key in diarization_result.keys():
    #       speaker = diarization_result[key]["speaker"]
    #       if speaker not in speakers:
    #         speakers.append(speaker)
    #     return len(speakers) > 1

def spoof_detection(audio_path: str) -> bool:
    # Objective: Detect whether the audio is spoofed or authentic.
    # Input: audio_path (str) - the path to the audio file.
    # Output: spoofed (bool) - True if the audio is spoofed, False otherwise.
    # Example usage:
    # If you want to check if an audio file is spoofed, you can call this function as follows:
    # spoofed = spoof_detection("path/to/audio/file.wav")

def SNR_estimation(audio_path: str) -> int:
    # Objective: Estimate the Signal-to-Noise Ratio (SNR) of an audio file.
    # Input: audio_path (str) - the path to the audio file.
    # Output: SNR (float) - the estimated SNR value.
    # This module can be used to estimate the SNR of an audio file, and can also be used for determining whether the audio is noisy or clean. See the example usage for more details.
    # Example usage 1:
    # If you want to estimate the SNR of an audio file, you can call this function as follows:
    # SNR = SNR_estimation("path/to/audio/file.wav")
    # Example usage 2:
    # If you want to choose an option that is most likely to be correct based on the SNR value, you can use the function as follows:
    # Suppose possible options are zero, ten, twenty, and clean.
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio files. options are the possible answers allowed.
    #     SNR = SNR_estimation(audio)
    #     # Since the results are deterministic, you do not need to query LLMs.
    #     # You only need to map the SNR value to the most closet value.
    #     if SNR < 5:
    #         answer = "zero"
    #     elif 5 <= SNR < 15:
    #         answer = "ten"
    #     elif 15 <= SNR < 25:
    #         answer = "twenty"
    #     else:
    #         answer = "clean"
    #     return answer
    # Example usage 3:
    # If you want to determine whether the audio file is noisy, you can use the function as follows:
    # def execute_command(audio) -> str:
    #     # audio is the input audio files. The output is "clean" if the audio is clean, "noisy" otherwise.
    #     SNR = SNR_estimation(audio)
    #     threshold = 15
    #     if SNR > threshold:
    #         return "clean"
    #     else:
    #         return "noisy"

def emotion_recognition(audio_path: str) -> str:
    # Objective: Recognize the emotions expressed in the audio.
    # Input: an audio
    # Output: the emotion label, e.g. angry, disgusted, fearful, happy, neutral, sad, surprised.
    # Please note that you need to query LLM (via query_llm function) to map the emotion labels to the possible answers, e.g. happiness maps to happy.
    # Example usage: 
    # You can identify the emotions expressed in the audio, with possible options: happy, sad, angry, fearful, disgust, surprised, through the following code:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     emotion = emotion_recognition(audio)
    #     # query based on the example question. You should modify this based on your specific task. 
    #     query = "Based on the emotion label predicted by emotion recognition model: f'{{emotion}}' and the possible options: f'{{options}}', what is the emotion expressed in this audio? Please only select one of the possible options."
    #     # use query_llm to map the emotion labels to the possible answers.
    #     answer = query_llm(query)
    #     return answer

def accent_recognition(audio_path: str) -> str:
    # Objective: Identify the accent of the speaker in the audio.
    # Input: an audio
    # Output: the accent (str). Currently supported accents: 'african', 'australia', 'bermuda', 'canada', 'england', 'hongkong', 'indian', 'ireland', 'malaysia', 'newzealand', 'philippines', 'scotland', 'singapore', 'southatlandtic', 'us', 'wales'.
    # Example usage: 
    # Suppose the task: identify the accent of the speaker in the audio. The possible options are: Australian, British, American, Indian, Irish.
    # This can be done by using this function as follows:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     accent = accent_recognition(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the accent: f'{{accent}}' and the options are f'{{options}}', what is the accent of the speaker in this audio?"
    #     # use query_llm to map the predicted results to the possible answers.
    #     answer = query_llm(query)
    #     return answer

def reverb_detection(audio_path: str) -> bool:
    # Objective: Detect if there is reverberation noise.
    # Input: audio_path (str) - the path to the audio file
    # Output: a sentence about whether it contains reverberation.
    # Please note that the output of this function is a descriptive sentence. You need to query LLM (via query_llm function) to extract the specific information from the caption.
    # Example usage:
    # If you want to detect reverberation noise in an audio file, you can call this function as follows:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     caption = reverb_detection(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the caption: f'{{caption}}' and the options are f'{{options}}', what is the option that matches the caption?"
    #     # use query_llm to map the predicted results to the possible answers.
    #     answer = query_llm(query)
    #     return answer

def human_speech_detection(audio_path: str) -> bool:
    # Objective: Detect whether the audio contains human speech.
    # Input: audio_path (str) - the path to the audio file.
    # Output: a descriptive sentence about the presence of the human speech in the audio. (str)
    # This module can only detect if the audio contains human speech or not. If you need to extract more information from the speech, you should use other modules.
    # Example usage:
    # If you want to detect if this audio contains human speech, you can call this function as follows:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     caption = human_speech_detection(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the sentence about the existence of human speech: f'{{caption}}' and the options are f'{{options}}', what is the option that matches the caption?"
    #     # use query_llm to map the predicted results to the possible answers.
    #     answer = query_llm(query)
    #     return answer

def distance_estimation(audio_path: str) -> str:
    # Objective: Estimate the distance of the speaker from the microphone.
    # Input: audio_path (str) - the path to the audio file.
    # Output: a descriptive sentence about the distance of the speaker in the audio. (str)
    # Example usage:
    # If you want to measure the speaker distance in an audio file, you can call this function as follows:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     caption = distance_estimation(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the distance information: f'{{caption}}' and the options are f'{{options}}', what is the option that matches the caption?"
    #     # use query_llm to map the predicted results to the possible answers.
    #     answer = query_llm(query)
    #     return answer

def stress_detection(audio_path: str) -> int:
    # Objective: Detect the position of stress in the spoken words.
    # Input: audio_path (str) - the path to the audio file.
    # Output: a sentence about the position of stress in the spoken words (str).
    # Note that the output of this function is a descriptive sentence. You need to query LLM (via query_llm function) to extract the specific information from the sentence.
    # Example usage:
    # If you want to detect the position of stress in an audio file, you can call this function as follows:
    # def execute_command(audio) -> str:
    #     # audio is the input audio file.
    #     stress_position = stress_detection(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the stress position: f'{{stress_position}}', what is the position of stress in the spoken words?"
    #     # use query_llm to map the predicted results to the possible answers.
    #     answer = query_llm(query)
    #     return answer

def language_recognition(audio_path: str) -> str:
    # Objective: Identify the language in the given speech.
    # Input: an audio
    # Output: the language, e.g. english, spanish, french, german, italian, dutch, portuguese, russian, japanese, korean, and arabic.
    # Example usage:
    # If you want to identify the language in the given speech and select the answer from the given option list, you can use this function as follows:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     # The following code will identify the language id in the given speech and query LLM to match the id with the langauge name.
    #     language = language_recognition(audio)
    #     # use query_llm to map the classification results to the possible answers.
    #     answer = query_llm("Based on the language id: f'{{language}}' and the options are f'{{options}}', what is the correct language?")
    #     return answer

def speaker_verification(audio_path1: str, audio_path2: str) -> bool:
    # Objective: Verify if two audio recordings originate from the same person.
    # Input: audio_path1 (str) - the path to the first audio file, audio_path2 (str) - the path to the second audio file.
    # Output: same_person (bool) - True if the recordings are from the same person, False otherwise.
    # Example usage:
    # If you want to verify if two audio files are from the same person, you can call this function as follows:
    # same_person = speaker_verification("path/to/audio/file1.wav", "path/to/audio/file2.wav")

def synthetic_speech_detection(audio_path: str) -> bool:
    # Objective: Detect whether the audio is synthetically generated or altered.
    # Input: audio_path (str) - the path to the audio file.
    # Output: a descriptive sentence about whether the audio is synthetic or authentic. (str)
    # Example usage:
    # If you want to check whether the audio is synthetic, you can call this function as follows:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     caption = synthetic_speech_detection(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the information: f'{{caption}}' and the options are f'{{options}}', what is the option that matches the caption?"
    #     # use query_llm to map the predicted results to the possible answers.
    #     answer = query_llm(query)
    #     return answer

def tune_classification(audio_path: str) -> str:
    # Objective: Classify the music tune as major or minor for piano and guitar.
    # Input: audio_path (str) - the path to the audio file.
    # Output: a descriptive sentence about the music information, which is a music caption. (str)
    # Example usage:
    # If you want to check the music information, you can call this function as follows:
    # def execute_command(audio, options) -> str:
    #     # audio is the input audio file. options is the allowed answer list.
    #     caption = tune_classification(audio)
    #     # query based on the example question. You should modify this based on your specific task.
    #     query = "Based on the music caption: f'{{caption}}' and the options are f'{{options}}', what is the option that matches the caption?"
    #     # use query_llm to map the predicted results to the possible answers.
    #     answer = query_llm(query)
    #     return answer